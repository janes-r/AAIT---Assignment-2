{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "how to use\n",
        "\n",
        "1 set up dataset and model parameter\\\n",
        "2 train a teacher model\\\n",
        "3 train student\\\n",
        "4 validate the student model on dataset"
      ],
      "metadata": {
        "id": "yE9CYvBEMRJD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#mount google drive"
      ],
      "metadata": {
        "id": "np6NNtFbMOEh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A-K6Q_eWEG1D",
        "outputId": "4b3caff7-217b-4f5a-c06c-421f5b64a920"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# dependencies"
      ],
      "metadata": {
        "id": "FRIWLqNWMSkL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "yJr8E__1EPjm"
      },
      "outputs": [],
      "source": [
        "import tarfile\n",
        "import os\n",
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from collections import Counter\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "import torch\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from collections import Counter\n",
        "\n",
        "from torch.utils.data import TensorDataset, ConcatDataset, DataLoader\n",
        "\n",
        "\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.models as models\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "\n",
        "from sklearn.mixture import GaussianMixture\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np\n",
        "\n",
        "from torchvision.transforms import Resize\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "from copy import deepcopy\n",
        "from torch.utils.data import TensorDataset, ConcatDataset, DataLoader\n",
        "import torchvision.transforms.functional as TF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AVazJpDTEXVg"
      },
      "source": [
        "#function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "zFLvRp_WEMfi"
      },
      "outputs": [],
      "source": [
        "def clean_data(device,confident_dataset_with_pseudo_labels,  model,batch_size = 64, some_confidence_threshold = 0.2):\n",
        "  model = model.eval()\n",
        "\n",
        "  dataloader = DataLoader(confident_dataset_with_pseudo_labels, batch_size=batch_size, shuffle=False)  # Adjust batch_size as per your requirement\n",
        "\n",
        "\n",
        "\n",
        "  model.to(device)\n",
        "  X, y = extract_features(dataloader, model)\n",
        "  X = X.cpu().numpy()\n",
        "  y = y.cpu().numpy()\n",
        "\n",
        "  # Normalize features\n",
        "  scaler = StandardScaler()\n",
        "  X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "  # Apply Gaussian Mixture Model\n",
        "  gmm = GaussianMixture(n_components=100, random_state=42)\n",
        "  gmm.fit(X_scaled)\n",
        "  probs = gmm.predict_proba(X_scaled)\n",
        "  hard_clusters = np.argmax(probs, axis=1)\n",
        "\n",
        "  confidence_scores = []\n",
        "  noisy_indices = []\n",
        "\n",
        "  for i in range(100):\n",
        "      cluster_indices = np.where(hard_clusters == i)[0]\n",
        "      cluster_labels = y[cluster_indices]\n",
        "      if len(cluster_labels) == 0:\n",
        "          continue\n",
        "      most_common_label = np.bincount(cluster_labels).argmax()\n",
        "      most_common_label_count = np.bincount(cluster_labels)[most_common_label]\n",
        "      total_count = len(cluster_labels)\n",
        "      confidence = most_common_label_count / total_count\n",
        "\n",
        "      # Check each data point in this cluster\n",
        "      for idx in cluster_indices:\n",
        "          if y[idx] != most_common_label and confidence < some_confidence_threshold:\n",
        "              noisy_indices.append(idx)\n",
        "\n",
        "  filtered_dataset = FilteredDataset(deepcopy(confident_dataset_with_pseudo_labels), noisy_indices)\n",
        "  unlabel_filtered_dataset = UnlabelDataset_filted(deepcopy(confident_dataset_with_pseudo_labels), noisy_indices)\n",
        "  print(len(filtered_dataset ))\n",
        "  print(len(unlabel_filtered_dataset))\n",
        "  return unlabel_filtered_dataset, filtered_dataset\n",
        "\n",
        "\n",
        "def extract_features(dataloader, model):\n",
        "    features = []\n",
        "    labels = []\n",
        "    with torch.no_grad():\n",
        "        i=0\n",
        "        for imgs, lbls in dataloader:\n",
        "            imgs, lbls = imgs.to(device), lbls.to(device)\n",
        "            if i%100 ==0:\n",
        "              print(f\"index {i}\")\n",
        "            i+=1\n",
        "            feats = model(imgs)\n",
        "            features.append(feats)\n",
        "            labels.append(lbls)\n",
        "    features = torch.cat(features)\n",
        "    labels = torch.cat(labels)\n",
        "    return features, labels\n",
        "\n",
        "\n",
        "def decompress_zip(file_path, extract_to_folder=None):\n",
        "    \"\"\"\n",
        "    Decompresses a zip file to the specified folder.\n",
        "\n",
        "    :param file_path: The path to the zip file.\n",
        "    :param extract_to_folder: The folder to extract the files into. If None, extracts in the same directory as the zip file.\n",
        "    :return: A list of file names that were extracted.\n",
        "    \"\"\"\n",
        "    if extract_to_folder is None:\n",
        "        extract_to_folder = os.path.dirname(file_path)\n",
        "\n",
        "    with tarfile.open(file_path, \"r:gz\") as tar:\n",
        "         tar.extractall(path=extract_to_folder)\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, data_path,csv_name, transform=None):\n",
        "        self.data_pth_abs = data_path\n",
        "        path = data_path+\"task2/train_data/\"+csv_name\n",
        "        #print(path)\n",
        "        self.data = pd.read_csv(path)\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.data_pth_abs+self.data.iloc[idx, 0]\n",
        "        #print(img_path)\n",
        "        #print(self.data)\n",
        "        label = self.data.iloc[idx, 1]\n",
        "\n",
        "        image = Image.open(img_path)\n",
        "\n",
        "        if image.mode == 'L':\n",
        "            image = image.convert('RGB')\n",
        "\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, label\n",
        "    def count_unique_labels(self):\n",
        "      return self.data.iloc[:, 1].nunique()\n",
        "\n",
        "class UnlabeledDataset(Dataset):\n",
        "    def __init__(self, data_path, transform=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            data_path (string): Path to the directory with all the images.\n",
        "            transform (callable, optional): Optional transform to be applied on a sample.\n",
        "        \"\"\"\n",
        "        self.data_path = data_path + \"task2/train_data/images/unlabeled\"\n",
        "        self.transform = transform\n",
        "        self.image_paths = [os.path.join(self.data_path, fname) for fname in os.listdir(self.data_path)]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.image_paths[idx]\n",
        "        image = Image.open(img_path)\n",
        "\n",
        "        if image.mode == 'L':\n",
        "            image = image.convert('RGB')\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def compute_metrics(outputs, labels):\n",
        "    _, preds = torch.max(outputs, 1)\n",
        "    preds = preds.cpu().numpy()\n",
        "    labels = labels.cpu().numpy()\n",
        "\n",
        "    accuracy = accuracy_score(labels, preds)\n",
        "    precision = precision_score(labels, preds, average='macro', zero_division=0)\n",
        "    recall = recall_score(labels, preds, average='macro', zero_division=0)\n",
        "    f1 = f1_score(labels, preds, average='macro', zero_division=0)\n",
        "\n",
        "    return accuracy, precision, recall, f1\n",
        "\n",
        "\n",
        "def smooth_labels(labels, num_classes, smoothing=0.1):\n",
        "    \"\"\"\n",
        "    Apply label smoothing.\n",
        "    :param labels: torch.Tensor, the labels.\n",
        "    :param num_classes: int, the number of classes.\n",
        "    :param smoothing: float, the smoothing factor.\n",
        "    \"\"\"\n",
        "    with torch.no_grad():\n",
        "        labels = labels.unsqueeze(1)\n",
        "        batch_size = labels.size(0)\n",
        "\n",
        "        true_dist = torch.zeros(batch_size, num_classes, device=labels.device)\n",
        "\n",
        "        true_dist.scatter_(1, labels, 1.0)\n",
        "\n",
        "        true_dist = true_dist * (1 - smoothing) + (smoothing / num_classes)\n",
        "    return true_dist\n",
        "\n",
        "def train(device, num_classes, class_weights, model, optimizer, criterion,  train_loader, val_loader, num_epochs, patience=3, path_to_save_model = \"'best_model.pth'\"):\n",
        "  print(device)\n",
        "  model.to(device)\n",
        "  best_val_accuracy = 0\n",
        "  best_val_loss = float('inf')\n",
        "  for epoch in range(num_epochs):\n",
        "      running_loss = 0.0\n",
        "      correct_train = 0\n",
        "      total_train = 0\n",
        "\n",
        "      epoch_loss = 0.0\n",
        "      epoch_acc = 0.0\n",
        "      epoch_precision = 0.0\n",
        "      epoch_recall = 0.0\n",
        "      epoch_f1 = 0.0\n",
        "      num_batches = 0\n",
        "\n",
        "\n",
        "\n",
        "      # Training Phase\n",
        "      model.train()\n",
        "      for inputs, labels in train_loader:\n",
        "          transform_to_size= Resize(size=(img_size,img_size), antialias=True)\n",
        "          inputs = transform_to_size(inputs)\n",
        "          inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "          optimizer.zero_grad()\n",
        "\n",
        "          outputs = model(inputs)\n",
        "\n",
        "          smoothed_labels = smooth_labels(labels, num_classes)\n",
        "          loss = criterion(outputs, smoothed_labels, class_weights)\n",
        "\n",
        "\n",
        "          acc, precision, recall, f1 = compute_metrics(outputs, labels)\n",
        "          epoch_acc += acc\n",
        "          epoch_precision += precision\n",
        "          epoch_recall += recall\n",
        "          epoch_f1 += f1\n",
        "          num_batches += 1\n",
        "\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "          loss_item = loss.item()\n",
        "          epoch_loss += loss_item\n",
        "          running_loss += loss_item\n",
        "          _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "\n",
        "\n",
        "          total_train += labels.size(0)\n",
        "          correct_train += (predicted == labels).sum().item()\n",
        "\n",
        "      epoch_loss /= num_batches\n",
        "      epoch_acc /= num_batches\n",
        "      epoch_precision /= num_batches\n",
        "      epoch_recall /= num_batches\n",
        "      epoch_f1 /= num_batches\n",
        "\n",
        "      print(f\"Epoch {epoch+1}, Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc:.4f}, Precision: {epoch_precision:.4f}, Recall: {epoch_recall:.4f}, F1 Score: {epoch_f1:.4f}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "      train_accuracy = 100 * correct_train / total_train\n",
        "\n",
        "      model.eval()\n",
        "      correct_val = 0\n",
        "      total_val = 0\n",
        "\n",
        "      running_loss1 = 0.0\n",
        "      with torch.no_grad():\n",
        "          for inputs, labels in val_loader:\n",
        "              transform_to_size=  transforms.Resize((img_size, img_size),antialias=True)\n",
        "              inputs = transform_to_size(inputs)\n",
        "              inputs, labels = inputs.to(device), labels.to(device)\n",
        "              outputs = model(inputs)\n",
        "              smoothed_labels = smooth_labels(labels, num_classes)\n",
        "              loss1 = criterion(outputs, smoothed_labels, class_weights)\n",
        "              running_loss1 += loss1.item()\n",
        "              _, predicted = torch.max(outputs.data, 1)\n",
        "              total_val += labels.size(0)\n",
        "              correct_val += (predicted == labels).sum().item()\n",
        "\n",
        "      val_accuracy = 100 * correct_val / total_val\n",
        "\n",
        "\n",
        "      val_loss = running_loss1 / len(val_loader)\n",
        "\n",
        "      if val_loss < best_val_loss:\n",
        "          best_val_loss = val_loss\n",
        "          patience_counter = 0\n",
        "          print(\"Improved validation loss. Saving model.\")\n",
        "          torch.save(model.state_dict(), 'best_model.pth')\n",
        "      else:\n",
        "          patience_counter += 1\n",
        "          print(f\"No improvement in validation loss for {patience_counter} epochs.\")\n",
        "\n",
        "      if patience_counter >= patience:\n",
        "          print(f\"Stopping early at epoch {epoch+1} due to no improvement in validation loss.\")\n",
        "          break\n",
        "\n",
        "      print(f'Epoch {epoch+1}/{num_epochs}, '\n",
        "            f'Train Loss: {running_loss/len(train_loader):.4f}, '\n",
        "            f'Train Accuracy: {train_accuracy:.2f}%, '\n",
        "            f'Train Loss: {running_loss1/len(val_loader):.4f}, '\n",
        "            f'Validation Accuracy: {val_accuracy:.2f}%')\n",
        "\n",
        "      if val_accuracy > best_val_accuracy:\n",
        "          best_val_accuracy = val_accuracy\n",
        "          print(\"best model\")\n",
        "          torch.save(model.state_dict(), path_to_save_model)\n",
        "\n",
        "  print('Finished Training')\n",
        "  return model\n",
        "\n",
        "def one_hot_encode(labels, num_classes, device):\n",
        "    one_hot = torch.eye(num_classes, device=device)\n",
        "    return one_hot[labels]\n",
        "\n",
        "def custom_cross_entropy(input, target, weight=None):\n",
        "    \"\"\"\n",
        "    Custom cross entropy loss for smoothed labels.\n",
        "    :param input: Logits from the model (before softmax).\n",
        "    :param target: Smoothed labels (2D tensor).\n",
        "    :param weight: Class weights (optional).\n",
        "    \"\"\"\n",
        "    log_probs = torch.nn.functional.log_softmax(input, dim=1)\n",
        "    loss = -torch.sum(target * log_probs, dim=1)\n",
        "\n",
        "    if weight is not None:\n",
        "        loss *= weight[target.argmax(dim=1)]\n",
        "\n",
        "    return loss.mean()\n",
        "\n",
        "def load_list(file_path ):\n",
        "  loaded_list = []\n",
        "\n",
        "  if not os.path.exists(file_path):\n",
        "      with open(file_path, 'w') as file:\n",
        "          file.write('')\n",
        "\n",
        "  with open(file_path, 'r') as file:\n",
        "      for line in file:\n",
        "          loaded_list.append(line.strip())\n",
        "  return loaded_list\n",
        "\n",
        "\n",
        "def evaluate_model(model, validation_loader, criterion,class_weights, num_classes=100):\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  model.eval()\n",
        "  correct_val = 0\n",
        "  total_val = 0\n",
        "\n",
        "  running_loss1 = 0.0\n",
        "  with torch.no_grad():\n",
        "      for inputs, labels in validation_loader:\n",
        "          transform_to_size=  transforms.Resize((img_size, img_size),antialias=True)\n",
        "          inputs = transform_to_size(inputs)\n",
        "          inputs, labels = inputs.to(device), labels.to(device)\n",
        "          outputs = model(inputs)\n",
        "          smoothed_labels = smooth_labels(labels, num_classes)\n",
        "          loss1 = criterion(outputs, smoothed_labels, class_weights)\n",
        "\n",
        "          running_loss1 += loss1.item()\n",
        "          _, predicted = torch.max(outputs.data, 1)\n",
        "          total_val += labels.size(0)\n",
        "          correct_val += (predicted == labels).sum().item()\n",
        "\n",
        "  val_accuracy = 100 * correct_val / total_val\n",
        "  print(f\"accuracy curent model on val {val_accuracy}\")\n",
        "  return val_accuracy\n",
        "\n",
        "\n",
        "def blend_weights(teacher_model, student_model, alpha=0.5):\n",
        "    \"\"\"\n",
        "    Blends the weights of the teacher and student models.\n",
        "\n",
        "    :param teacher_model: The teacher model.\n",
        "    :param student_model: The student model.\n",
        "    :param alpha: Blending factor, a value between 0 and 1.\n",
        "                  alpha=1 means full student weights, alpha=0 means full teacher weights.\n",
        "    \"\"\"\n",
        "    alpha = max(0, min(1, alpha))\n",
        "    for teacher_param, student_param in zip(teacher_model.parameters(), student_model.parameters()):\n",
        "        teacher_param.data = alpha * student_param.data + (1 - alpha) * teacher_param.data\n",
        "\n",
        "\n",
        "class FilteredDataset(Dataset):\n",
        "    def __init__(self, original_dataset, noisy_indices):\n",
        "        self.original_dataset = original_dataset\n",
        "        self.noisy_indices = set(noisy_indices)\n",
        "        self.filtered_indices = [i for i in range(len(original_dataset)) if i not in self.noisy_indices]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.filtered_indices)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "      original_idx = self.filtered_indices[idx]\n",
        "      data, label = self.original_dataset[original_idx]\n",
        "      return data.clone().detach().requires_grad_(True), label\n",
        "\n",
        "\n",
        "class UnlabelDataset_filted(Dataset):\n",
        "    def __init__(self, original_dataset, noisy_indices):\n",
        "        self.original_dataset = original_dataset\n",
        "        self.noisy_indices = set(noisy_indices)\n",
        "        self.filtered_indices = [i for i in range(len(original_dataset)) if i in self.noisy_indices]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.filtered_indices)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Map the new index to the original index\n",
        "        original_idx = self.filtered_indices[idx]\n",
        "        data, label = self.original_dataset[idx]\n",
        "        return data.clone().detach().requires_grad_(True)\n",
        "\n",
        "\n",
        "def calculate_class_weights(dataset, num_classes=100):\n",
        "    # Count the number of samples for each class\n",
        "    class_counts = Counter([label for _, label in dataset])\n",
        "\n",
        "    # Calculate the weight for each class\n",
        "    num_samples = len(dataset)\n",
        "    class_weights = {class_id : num_samples / (num_classes * count)\n",
        "                     for class_id, count in class_counts.items()}\n",
        "\n",
        "    # Ensure weights are set for all classes, not just those in class_counts\n",
        "    for cls in range(num_classes):\n",
        "        class_weights.setdefault(cls, 1.0)\n",
        "\n",
        "    # Convert to a list where the index corresponds to the class ID\n",
        "    weights = [class_weights[i] for i in range(num_classes)]\n",
        "\n",
        "    return torch.tensor(weights, dtype=torch.float)\n",
        "\n",
        "\n",
        "'''\n",
        "class CustomDataset_t_s(Dataset):\n",
        "    def __init__(self, file_paths=None, labels=None, transform=None, save_dir='dataset_images'):\n",
        "        self.file_paths = []  # List of file paths\n",
        "        self.labels = []\n",
        "        self.transform = transform\n",
        "        self.save_dir = save_dir\n",
        "        if not os.path.exists(self.save_dir):\n",
        "            os.makedirs(self.save_dir)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.file_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = Image.open(self.file_paths[idx])\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        label = self.labels[idx]\n",
        "        return image, int(label)\n",
        "\n",
        "    def add_data(self, new_data, new_label):\n",
        "        # Generate a unique filename for the new image\n",
        "        new_filename = f'image_{len(self.file_paths)}.png'\n",
        "        new_file_path = os.path.join(self.save_dir, new_filename)\n",
        "        new_data_image = TF.to_pil_image(new_data)\n",
        "        # Save the new image to disk\n",
        "        #print(new_file_path)\n",
        "        new_data_image.save(new_file_path)\n",
        "\n",
        "        # Update the dataset\n",
        "        self.file_paths.append(new_file_path)\n",
        "        self.labels = np.append(self.labels, new_label)\n",
        "\n",
        "    def __del__(self):\n",
        "        # Cleanup: Delete the images when the dataset object is deleted\n",
        "        for file_path in self.file_paths:\n",
        "            if os.path.exists(file_path):\n",
        "                os.remove(file_path)\n",
        "        # Optionally, delete the directory if it's empty\n",
        "        print(\"Dataset images and directory have been cleaned up.\")\n",
        "'''\n",
        "\n",
        "class CustomDataset_t_s(Dataset):\n",
        "    def __init__(self, data, labels, transform=None):\n",
        "        self.data = data\n",
        "        self.labels = labels.cpu().numpy()\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        data_point = self.data[idx]\n",
        "        label = self.labels[idx]\n",
        "        image_tensor = data_point\n",
        "        if image_tensor.shape[0] < image_tensor.shape[2]:\n",
        "          image_tensor = image_tensor.permute(1, 2, 0)\n",
        "\n",
        "        # Convert to numpy array\n",
        "        image_array = image_tensor.detach().numpy()\n",
        "\n",
        "        # Convert to PIL Image\n",
        "        image = Image.fromarray((image_array * 255).astype('uint8'))\n",
        "\n",
        "\n",
        "\n",
        "        if image.mode == 'L':\n",
        "            image = image.convert('RGB')\n",
        "\n",
        "\n",
        "        #print(\"sdfsdfsd\")\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label\n",
        "\n",
        "\n",
        "def calculate_class_thresholds(dataset, b_alpha):\n",
        "    # Dummy function to calculate per-class thresholds based on dataset and b_alpha\n",
        "    # Replace with actual logic\n",
        "    num_classes = 100  # Update with actual number of classes\n",
        "    return [0.7 for _ in range(num_classes)]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#dataset"
      ],
      "metadata": {
        "id": "_k_9eOpnePaS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "data_pth = \"/content/gdrive/MyDrive/sejour_aca/aait/task2.tar.gz\"\n",
        "decompress_zip(data_pth,\"./\")"
      ],
      "metadata": {
        "id": "GmUEgZwpeS7t"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T0883TNlEYv_"
      },
      "source": [
        "# train teacher"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## dataset teacher"
      ],
      "metadata": {
        "id": "_70jFaKSJQNH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### parameter"
      ],
      "metadata": {
        "id": "ecuGMTJyHgKv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TEQKQOcJKe-u"
      },
      "outputs": [],
      "source": [
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((img_size, img_size)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "img_size = 224\n",
        "batch_size = 32  # Adjust as needed\n",
        "# Set the random seed for reproducibility\n",
        "random_seed = 42\n",
        "train_proportion = 0.8  # 80% for training\n",
        "\n",
        "\n",
        "unlabeled_dataset = UnlabeledDataset(data_path='/content/', transform=transform)\n",
        "c = CustomDataset(data_path='/content/', csv_name = 'annotations.csv', transform=transform)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## seting up dataset"
      ],
      "metadata": {
        "id": "YAT9OCCZIO72"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(random_seed)"
      ],
      "metadata": {
        "id": "_4nLkekmILVm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WW5PhZA1HLEY"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "file_path = '/content/indices_list.txt'\n",
        "indices_list = load_list(file_path )\n",
        "\n",
        "file_path = '/content/noisy_indices2.txt'\n",
        "noise_list = load_list(file_path )\n",
        "\n",
        "file_path = '/content/pseudo_labels_list.txt'\n",
        "pseudo_labels_list = load_list(file_path )\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pixA6WzLKyqE"
      },
      "outputs": [],
      "source": [
        "#indices_list = [int(i) for i in indices_list]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "znzzkFgiihJT"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "eval_proportion = 1 - train_proportion\n",
        "\n",
        "train_size = int(train_proportion * len(labeled_dataset ))\n",
        "val_size = len(labeled_dataset) - train_size\n",
        "\n",
        "\n",
        "train_dataset, val_dataset = random_split(labeled_dataset , [train_size, val_size])\n",
        "\n",
        "\n",
        "\n",
        "# Create DataLoaders for training and evaluation sets\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0wEbPmvVFJiP"
      },
      "source": [
        "## train teacher"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pFWxLV3ZFvcx"
      },
      "source": [
        "### parameter"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "path_model_teacher = './best_model.pth'\n",
        "num_classes = 100\n",
        "num_epochs = 30\n",
        "\n",
        "\n",
        "#model choice\n",
        "resnet50 = models.resnet50(pretrained=True).to(device)\n",
        "resnet50.fc = nn.Linear(resnet50.fc.in_features, num_classes)"
      ],
      "metadata": {
        "id": "iD_OjpdDIV0t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### main"
      ],
      "metadata": {
        "id": "tt12jkcIIujR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TloxRa7wE8pA",
        "outputId": "c5eec231-0cc7-4286-dba1-ac02fea217ef"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n",
            "Epoch 1, Loss: 0.0000, Accuracy: 0.0513, Precision: 0.0249, Recall: 0.0368, F1 Score: 0.0266\n",
            "Improved validation loss. Saving model.\n",
            "Epoch 1/30, Train Loss: 4.3010, Train Accuracy: 5.13%, Train Loss: 3.9600, Validation Accuracy: 10.11%\n",
            "best model\n",
            "Epoch 2, Loss: 0.0000, Accuracy: 0.1327, Precision: 0.0773, Recall: 0.0865, F1 Score: 0.0775\n",
            "Improved validation loss. Saving model.\n",
            "Epoch 2/30, Train Loss: 3.8048, Train Accuracy: 13.26%, Train Loss: 3.7180, Validation Accuracy: 16.18%\n",
            "best model\n",
            "Epoch 3, Loss: 0.0000, Accuracy: 0.1915, Precision: 0.1174, Recall: 0.1251, F1 Score: 0.1162\n",
            "Improved validation loss. Saving model.\n",
            "Epoch 3/30, Train Loss: 3.5497, Train Accuracy: 19.18%, Train Loss: 3.4822, Validation Accuracy: 22.32%\n",
            "best model\n",
            "Epoch 4, Loss: 0.0000, Accuracy: 0.2590, Precision: 0.1642, Recall: 0.1680, F1 Score: 0.1603\n",
            "Improved validation loss. Saving model.\n",
            "Epoch 4/30, Train Loss: 3.2948, Train Accuracy: 25.92%, Train Loss: 3.2538, Validation Accuracy: 28.35%\n",
            "best model\n",
            "Epoch 5, Loss: 0.0000, Accuracy: 0.3119, Precision: 0.2039, Recall: 0.2070, F1 Score: 0.1993\n",
            "Improved validation loss. Saving model.\n",
            "Epoch 5/30, Train Loss: 3.1025, Train Accuracy: 31.18%, Train Loss: 3.1643, Validation Accuracy: 30.86%\n",
            "best model\n",
            "Epoch 6, Loss: 0.0000, Accuracy: 0.3636, Precision: 0.2449, Recall: 0.2460, F1 Score: 0.2388\n",
            "Improved validation loss. Saving model.\n",
            "Epoch 6/30, Train Loss: 2.9106, Train Accuracy: 36.39%, Train Loss: 3.0679, Validation Accuracy: 33.74%\n",
            "best model\n",
            "Epoch 7, Loss: 0.0000, Accuracy: 0.4181, Precision: 0.2906, Recall: 0.2905, F1 Score: 0.2831\n",
            "Improved validation loss. Saving model.\n",
            "Epoch 7/30, Train Loss: 2.7373, Train Accuracy: 41.80%, Train Loss: 2.9686, Validation Accuracy: 37.03%\n",
            "best model\n",
            "Epoch 8, Loss: 0.0000, Accuracy: 0.4693, Precision: 0.3347, Recall: 0.3332, F1 Score: 0.3265\n",
            "Improved validation loss. Saving model.\n",
            "Epoch 8/30, Train Loss: 2.5420, Train Accuracy: 46.90%, Train Loss: 2.8939, Validation Accuracy: 38.90%\n",
            "best model\n",
            "Epoch 9, Loss: 0.0000, Accuracy: 0.5317, Precision: 0.3945, Recall: 0.3909, F1 Score: 0.3846\n",
            "Improved validation loss. Saving model.\n",
            "Epoch 9/30, Train Loss: 2.3481, Train Accuracy: 53.15%, Train Loss: 2.8244, Validation Accuracy: 40.89%\n",
            "best model\n",
            "Epoch 10, Loss: 0.0000, Accuracy: 0.6009, Precision: 0.4600, Recall: 0.4563, F1 Score: 0.4506\n",
            "No improvement in validation loss for 1 epochs.\n",
            "Epoch 10/30, Train Loss: 2.1337, Train Accuracy: 60.10%, Train Loss: 2.8835, Validation Accuracy: 41.17%\n",
            "best model\n",
            "Epoch 11, Loss: 0.0000, Accuracy: 0.6838, Precision: 0.5502, Recall: 0.5459, F1 Score: 0.5408\n",
            "No improvement in validation loss for 2 epochs.\n",
            "Epoch 11/30, Train Loss: 1.8906, Train Accuracy: 68.40%, Train Loss: 2.9629, Validation Accuracy: 39.97%\n",
            "Epoch 12, Loss: 0.0000, Accuracy: 0.7702, Precision: 0.6539, Recall: 0.6488, F1 Score: 0.6453\n",
            "No improvement in validation loss for 3 epochs.\n",
            "Stopping early at epoch 12 due to no improvement in validation loss.\n",
            "Finished Training\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "ResNet(\n",
              "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu): ReLU(inplace=True)\n",
              "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  (layer1): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (3): Bottleneck(\n",
              "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (3): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (4): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (5): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  (fc): Linear(in_features=2048, out_features=100, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 105,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "class_weights = calculate_class_weights(labeled_dataset )\n",
        "\n",
        "optimizer = optim.Adam(resnet50.parameters(), lr=0.001)\n",
        "\n",
        "\n",
        "class_weights = class_weights.to(device)\n",
        "criterion = custom_cross_entropy\n",
        "train(device, num_classes,class_weights, resnet50, optimizer, criterion,  train_loader, val_loader, num_epochs, patience=3, path_to_save_model =path_model_teacher)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1756KYGFeoqN"
      },
      "source": [
        "#analyse dataset is balanced"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "dataset to analyse"
      ],
      "metadata": {
        "id": "P7bPp4tuLch0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = labeled_dataset"
      ],
      "metadata": {
        "id": "h5dHz0I7Lfwy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## code"
      ],
      "metadata": {
        "id": "q8Tpef0VLsBa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sMYnRTyJepA6"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Count the frequency of each class\n",
        "class_counts = {}\n",
        "for _, label in filtered_dataset:\n",
        "    if label in class_counts:\n",
        "        class_counts[label] += 1\n",
        "    else:\n",
        "        class_counts[label] = 1\n",
        "\n",
        "# Sort the classes if they are not numeric or if you want them in order\n",
        "class_labels = sorted(class_counts.keys())\n",
        "\n",
        "# Get counts in the same order as class labels\n",
        "counts = [class_counts[label] for label in class_labels]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "dRXXBjd6erZR",
        "outputId": "3aa8a7c3-c6c0-48a0-9e6d-7d379939fbeb"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "# Create a bar chart\n",
        "plt.bar(class_labels, counts)\n",
        "plt.xlabel('Class')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Class Distribution in Dataset')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3QVF0MO1Ryk7"
      },
      "source": [
        "# train student with teacher"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "s4iECS1R8dtI"
      },
      "outputs": [],
      "source": [
        "random_seed = 42\n",
        "batch_size = 32  # Adjust as needed\n",
        "batch_size_clean = 64\n",
        "model_path = \"/content/gdrive/MyDrive/model/best_model2_task2.pth\"\n",
        "\n",
        "\n",
        "student_path = 'best_model.pth'\n",
        "student_epochs = 5\n",
        "img_size = 224\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "labeled_dataset = CustomDataset(data_path='/content/', csv_name = 'annotations.csv', transform=transform)\n",
        "confidence_threshold_clean_data = 0.7\n",
        "\n",
        "# Define the proportions for splitting\n",
        "train_proportion = 0.8\n",
        "eval_proportion = 1 - train_proportion\n",
        "\n",
        "nb_classes = 100\n",
        "\n",
        "num_iterations = 15\n",
        "\n",
        "initial_threshold = 0.70\n",
        "threshold_increment = 0.05\n",
        "threshold_decrement = 0.03\n",
        "max_threshold = 0.95\n",
        "min_threshold = 0.60\n",
        "\n",
        "alpha_student = 0.8\n",
        "alpha_teacher = 0.2\n",
        "\n",
        "b_alpha = 0.1 # alpha for class thresholds\n",
        "\n",
        "lr_student = 0.001\n",
        "\n",
        "patience_training = 4\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RPVOGkVtyVAN",
        "outputId": "ff02a96d-98d2-4f0c-dfd0-ea483cd6bf4e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "index 0\n",
            "index 100\n",
            "index 200\n",
            "index 300\n",
            "index 400\n",
            "index 500\n",
            "index 600\n",
            "index 700\n",
            "index 800\n",
            "index 900\n",
            "index 1000\n",
            "index 1100\n",
            "index 1200\n",
            "index 1300\n",
            "index 1400\n",
            "index 1500\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "from copy import deepcopy\n",
        "from torch.utils.data import TensorDataset, ConcatDataset, DataLoader\n",
        "\n",
        "torch.manual_seed(random_seed)\n",
        "\n",
        "teacher_model = models.resnet50(pretrained=False)\n",
        "teacher_model.fc = nn.Linear(teacher_model.fc.in_features, nb_classes)\n",
        "\n",
        "\n",
        "teacher_model.load_state_dict(torch.load(model_path))\n",
        "\n",
        "unlabeled_dataset, labeled_dataset = clean_data(device, labeled_dataset, teacher_model, batch_size = batch_size, some_confidence_threshold = confidence_threshold_clean_data)\n",
        "\n",
        "train_loader_unlabeled = DataLoader(unlabeled_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "train_size = int(train_proportion * len(labeled_dataset))\n",
        "val_size = len(labeled_dataset) - train_size\n",
        "\n",
        "train_dataset, val_dataset = random_split(labeled_dataset, [train_size, val_size])\n",
        "\n",
        "\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
        "\n",
        "teacher_model.to(device)\n",
        "\n",
        "confidence_threshold = initial_threshold\n",
        "previous_accuracy = 0\n",
        "\n",
        "\n",
        "class_weights = calculate_class_weights(labeled_dataset)\n",
        "class_weights = class_weights.to(device)\n",
        "criterion = custom_cross_entropy\n",
        "\n",
        "student_model = models.resnet50(pretrained=True)\n",
        "student_model.fc = nn.Linear(student_model.fc.in_features, nb_classes) # 100 classes\n",
        "\n",
        "\n",
        "student_model.to(device)\n",
        "blend_weights(student_model,teacher_model, alpha=alpha_teacher)\n",
        "optimizer = optim.Adam(student_model.parameters(), lr=lr_student)\n",
        "\n",
        "\n",
        "\n",
        "class_specific_thresholds = calculate_class_thresholds(labeled_dataset, b_alpha)\n",
        "\n",
        "unique_indices = set()\n",
        "for iteration in range(num_iterations):\n",
        "    # Generate pseudo-labels for the unlabeled dataset\n",
        "    teacher_model.eval()\n",
        "    student_model.eval()\n",
        "    # Evaluate the model on the validation set\n",
        "    current_accuracy = evaluate_model(teacher_model, val_loader,criterion, class_weights)  # Implement evaluate_model function\n",
        "\n",
        "    # Adjust the threshold based on validation performance\n",
        "    if current_accuracy > previous_accuracy:\n",
        "        confidence_threshold = min(confidence_threshold + threshold_increment, max_threshold)\n",
        "    elif current_accuracy <= previous_accuracy:\n",
        "        confidence_threshold = max(confidence_threshold - threshold_decrement, min_threshold)\n",
        "\n",
        "\n",
        "    pseudo_labels_list = []\n",
        "    indices_list = []\n",
        "\n",
        "    accumulated_pseudo_labels = []\n",
        "    accumulated_unlabeled_data = []\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, inputs in enumerate(train_loader_unlabeled):\n",
        "            transform_to_size=  transforms.Resize((img_size, img_size),antialias=True)\n",
        "            inputs = transform_to_size(inputs)\n",
        "            inputs = inputs.to(device)\n",
        "            outputs = teacher_model(inputs)\n",
        "            probabilities = torch.nn.functional.softmax(outputs, dim=1)\n",
        "            max_probs, predicted = torch.max(probabilities, 1)\n",
        "\n",
        "            for class_idx in range(100):  # Assuming 100 classes\n",
        "                class_threshold = class_specific_thresholds[class_idx]\n",
        "                class_mask = (max_probs > class_threshold)\n",
        "                confident_indices = class_mask.nonzero(as_tuple=False).view(-1)\n",
        "\n",
        "                confident_predictions = predicted[confident_indices].cpu().numpy()\n",
        "                pseudo_labels_list.extend(confident_predictions)\n",
        "                indices_list.extend([int(ele + i * inputs.shape[0]) for ele in confident_indices.cpu().numpy()])\n",
        "\n",
        "\n",
        "    #initial_size = len(indices_list)  # An initial estimate\n",
        "    #num_features = [3, 224, 224]  # For RGB images of size 224x224\n",
        "    #del confident_dataset_with_pseudo_labels\n",
        "    #confident_dataset_with_pseudo_labels = CustomDataset_t_s(initial_size, num_features, transform)\n",
        "    '''\n",
        "    confident_dataset_with_pseudo_labels = CustomDataset_t_s(transform=transform, save_dir='/content/temp_img/')\n",
        "\n",
        "    for idx, pseudo_label in zip(indices_list, pseudo_labels_list):\n",
        "        confident_dataset_with_pseudo_labels.add_data(unlabeled_dataset[idx], pseudo_label)\n",
        "        #confident_dataset_with_pseudo_labels.add_data(unlabeled_dataset[idx], pseudo_label)\n",
        "    '''\n",
        "\n",
        "    for idx, pseudo_label in zip(indices_list, pseudo_labels_list):\n",
        "        accumulated_pseudo_labels.append(pseudo_label)\n",
        "        accumulated_unlabeled_data.append(unlabeled_dataset[idx])\n",
        "\n",
        "\n",
        "    confident_dataset_with_pseudo_labels = CustomDataset_t_s(accumulated_unlabeled_data, torch.tensor(accumulated_pseudo_labels), transform=transform)\n",
        "\n",
        "\n",
        "    #confident_dataset_with_pseudo_labels = CustomDataset_t_s(accumulated_unlabeled_data, torch.tensor(accumulated_pseudo_labels))\n",
        "    len(f\"len confidence {confident_dataset_with_pseudo_labels}\")\n",
        "    #del accumulated_unlabeled_data, indices_list,pseudo_labels_list\n",
        "    import gc\n",
        "    gc.collect()\n",
        "    if len(confident_dataset_with_pseudo_labels)<10:\n",
        "      confidence_threshold-=0.05\n",
        "      continue\n",
        "    # clean the data\n",
        "    unlabeled_dataset, data_plus_cleaned = clean_data(device, confident_dataset_with_pseudo_labels, teacher_model, batch_size = batch_size_clean, some_confidence_threshold = current_accuracy/100/3*2)\n",
        "\n",
        "    train_loader_unlabeled = DataLoader(unlabeled_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    combined_dataset = ConcatDataset([train_dataset, data_plus_cleaned])\n",
        "\n",
        "\n",
        "    print(f\"new dataset of len {len(combined_dataset)}\")\n",
        "    print(f\"dataset increase by {len(combined_dataset)/len(train_dataset)*100}%\")\n",
        "\n",
        "\n",
        "    combined_train_loader = DataLoader(combined_dataset, batch_size=batch_size, shuffle=True)\n",
        "    #combined_val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "\n",
        "    #val_loader = combined_val_loader\n",
        "    # Train the student model on the combined dataset\n",
        "\n",
        "\n",
        "\n",
        "    print(f\"Iteration {iteration+1}\")\n",
        "    criterion = custom_cross_entropy\n",
        "\n",
        "    train(device,nb_classes, class_weights, student_model, optimizer, criterion,  combined_train_loader, val_loader, student_epochs, patience=patience_training, path_to_save_model= student_path)\n",
        "    student_model.load_state_dict(torch.load(student_path))\n",
        "\n",
        "    # Update the teacher model\n",
        "    blend_weights(teacher_model,student_model, alpha=alpha_teacher)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Uk5voXeV9TJS"
      },
      "outputs": [],
      "source": [
        "!cp /content/best_model.pth /content/gdrive/MyDrive/model/best_model_task1_final.pth"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-j0_YrflvLD0"
      },
      "source": [
        "#validation of model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fHwifP8KvehE"
      },
      "source": [
        "##functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DBJfPJ0BvfIO"
      },
      "outputs": [],
      "source": [
        "class UnlabeledDataset1(Dataset):\n",
        "    def __init__(self, data_path, transform=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            data_path (string): Path to the directory with all the images.\n",
        "            transform (callable, optional): Optional transform to be applied on a sample.\n",
        "        \"\"\"\n",
        "        self.data_path = data_path + \"task1/val_data/\"\n",
        "        #self.data_path = data_path + \"task1/train_data/images/unlabeled\"\n",
        "        self.transform = transform\n",
        "        self.image_paths = [os.path.join(self.data_path, fname) for fname in os.listdir(self.data_path)]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.image_paths[idx]\n",
        "        image = Image.open(img_path)\n",
        "\n",
        "        if image.mode == 'L':\n",
        "            image = image.convert('RGB')\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N01Z_jvjvemW"
      },
      "source": [
        "#main validation"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "parameter"
      ],
      "metadata": {
        "id": "pxvI9roPGXMo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = '/content/best_model.pth'\n",
        "dataset = UnlabeledDataset1(data_path='/content/', transform=transform)\n",
        "batch_s = 500\n",
        "output_csv_final = 'sorted_predicted_results3.csv'"
      ],
      "metadata": {
        "id": "F8PoQaRXGT1E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## validation code"
      ],
      "metadata": {
        "id": "hRV8ky4pGZPZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WVTPK3GsvLLr"
      },
      "outputs": [],
      "source": [
        "import torchvision.models as models\n",
        "\n",
        "model = models.resnet50(pretrained=False)\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = models.resnet50(pretrained=False).to(device)\n",
        "model.fc = nn.Linear(model.fc.in_features, 100)\n",
        "\n",
        "model.load_state_dict(torch.load(model_path))\n",
        "ensemble_model = model\n",
        "\n",
        "ensemble_model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "naecA9o4wGkJ"
      },
      "outputs": [],
      "source": [
        "ensemble_model = teacher_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DCoDjRs1vllt",
        "outputId": "ee155694-4a7b-48e8-eeee-316cd351d5f6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Classifying images: 100%|██████████| 10/10 [00:21<00:00,  2.14s/ images]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Results saved to ./predicted_results.csv\n",
            "Sorted results saved to sorted_predicted_results.csv\n",
            "Sorted results saved to sorted_predicted_results.csv\n",
            "Sorted results saved to sorted_predicted_results2.csv\n",
            "Updated results with \"img\" prefix saved to sorted_predicted_results3.csv\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((img_size, img_size)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "data_loader = DataLoader(dataset, batch_size=batch_s, shuffle=False)\n",
        "\n",
        "\n",
        "model = ensemble_model\n",
        "\n",
        "model.eval()\n",
        "\n",
        "image_paths = []\n",
        "predicted_classes = []\n",
        "\n",
        "\n",
        "total = 0\n",
        "for batch in tqdm(data_loader, desc='Classifying images', unit=' images'):\n",
        "    inputs = batch.to(device)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(inputs)\n",
        "    _, predicted = torch.max(outputs.data, 1)\n",
        "    next = total + batch_s\n",
        "    image_paths.extend(dataset.image_paths[total: next])\n",
        "    total = total+ batch_s\n",
        "    #print(len(predicted.tolist()))\n",
        "    predicted_classes.extend(predicted.tolist())\n",
        "\n",
        "result_df = pd.DataFrame({'Image_Path': image_paths, 'Predicted_Class': predicted_classes})\n",
        "\n",
        "result_csv = './predicted_results.csv'\n",
        "result_df.to_csv(result_csv, index=False)\n",
        "\n",
        "print(f'Results saved to {result_csv}')\n",
        "import pandas as pd\n",
        "\n",
        "# Read the CSV file\n",
        "input_csv = 'predicted_results.csv'\n",
        "df = pd.read_csv(input_csv)\n",
        "\n",
        "sorted_df = df.sort_values(by='Image_Path')\n",
        "\n",
        "output_csv = 'sorted_predicted_results.csv'\n",
        "sorted_df.to_csv(output_csv, index=False)\n",
        "\n",
        "print(f'Sorted results saved to {output_csv}')\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "input_csv = 'predicted_results.csv'\n",
        "df = pd.read_csv(input_csv)\n",
        "\n",
        "\n",
        "df['Image_Number'] = df['Image_Path'].str.extract('(\\d+)\\.jpeg')\n",
        "\n",
        "df['Image_Number'] = pd.to_numeric(df['Image_Number'])\n",
        "\n",
        "sorted_df = df.sort_values(by='Image_Number')\n",
        "\n",
        "\n",
        "output_csv = 'sorted_predicted_results.csv'\n",
        "\n",
        "sorted_df.to_csv(output_csv, index=False)\n",
        "print(f'Sorted results saved to {output_csv}')\n",
        "\n",
        "input_csv = 'sorted_predicted_results.csv'\n",
        "df = pd.read_csv(input_csv)\n",
        "\n",
        "df['Image_Path'] = df['Image_Path'].apply(lambda x: x.split('/')[-1])\n",
        "output_csv = 'sorted_predicted_results2.csv'\n",
        "df.to_csv(output_csv, index=False)\n",
        "\n",
        "print(f'Sorted results saved to {output_csv}')\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "input_csv = 'sorted_predicted_results2.csv'\n",
        "df = pd.read_csv(input_csv)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "df.rename(columns={'Image_Path': 'sample', 'Predicted_Class': 'label'}, inplace=True)\n",
        "\n",
        "\n",
        "\n",
        "df = df.drop(columns=['Image_Number'])\n",
        "df.to_csv(output_csv, index=False)\n",
        "\n",
        "print(f'Updated results with \"img\" prefix saved to {output_csv_final}')\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "np6NNtFbMOEh",
        "FRIWLqNWMSkL",
        "T0883TNlEYv_",
        "_70jFaKSJQNH",
        "ecuGMTJyHgKv",
        "YAT9OCCZIO72",
        "0wEbPmvVFJiP",
        "pFWxLV3ZFvcx",
        "tt12jkcIIujR",
        "1756KYGFeoqN",
        "q8Tpef0VLsBa",
        "fHwifP8KvehE",
        "N01Z_jvjvemW",
        "hRV8ky4pGZPZ"
      ],
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}